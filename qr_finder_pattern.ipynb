{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qr finder pattern.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L6pCmkJrUC9g",
        "I_xcqQZKYzAj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/splendiferousnoctifer/qrFinderPattern/blob/codeCreation/qr_finder_pattern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzRKNe1iSlNW"
      },
      "source": [
        "# QR Finder Patterns from Video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6pCmkJrUC9g"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj9YcAnsT4B_"
      },
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09b_0FAnUa9y"
      },
      "source": [
        "def imshow(image, *args, **kwargs):\n",
        "    \"\"\"A replacement for cv2.imshow() for use in Jupyter notebooks using matplotlib.\n",
        "\n",
        "        Args:\n",
        "          image : np.ndarray. shape (N, M) or (N, M, 1) is an NxM grayscale image. shape\n",
        "            (N, M, 3) is an NxM BGR color image. \n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "      # Height, width, channels\n",
        "      # Assume BGR, do a conversion  \n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Draw the image\n",
        "    plt.imshow(image, *args, **kwargs)\n",
        "    # We'll also disable drawing the axes and tick marks in the plot, since it's actually an image\n",
        "    plt.axis('off')\n",
        "    # Make sure it outputs\n",
        "    # plt.show()\n",
        "    \n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaZIOR4WaT64"
      },
      "source": [
        "## QR Finder Pattern extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BLUR_VALUE = 3\n",
        "SQUARE_TOLERANCE = 0.15\n",
        "AREA_TOLERANCE = 0.15\n",
        "DISTANCE_TOLERANCE = 0.25\n",
        "WARP_DIM = 300\n",
        "SMALL_DIM = 29\n",
        "\n",
        "\n",
        "def count_children(hierarchy, parent, inner=False):\n",
        "    \"\"\"\n",
        "    Counts children in hierarchy to check if \n",
        "    \"\"\"\n",
        "\n",
        "    if parent == -1:\n",
        "        return 0\n",
        "    elif not inner:\n",
        "        return count_children(hierarchy, hierarchy[parent][2], True)\n",
        "    return 1 + count_children(hierarchy, hierarchy[parent][0], True) + count_children(hierarchy, hierarchy[parent][2], True)\n",
        "\n",
        "\n",
        "def has_square_parent(hierarchy, squares, parent):\n",
        "    if hierarchy[parent][3] == -1:\n",
        "        return False\n",
        "    if hierarchy[parent][3] in squares:\n",
        "        return True\n",
        "    return has_square_parent(hierarchy, squares, hierarchy[parent][3])\n",
        "\n",
        "\n",
        "def get_center(c):\n",
        "    m = cv2.moments(c)\n",
        "    return [int(m[\"m10\"] / m[\"m00\"]), int(m[\"m01\"] / m[\"m00\"])]\n",
        "\n",
        "\n",
        "def get_angle(p1, p2):\n",
        "    x_diff = p2[0] - p1[0]\n",
        "    y_diff = p2[1] - p1[1]\n",
        "    return math.degrees(math.atan2(y_diff, x_diff))\n",
        "\n",
        "\n",
        "def get_midpoint(p1, p2):\n",
        "    return [(p1[0] + p2[0]) / 2, (p1[1] + p2[1]) / 2]\n",
        "\n",
        "\n",
        "def get_farthest_points(contour, center):\n",
        "    distances = []\n",
        "    distances_to_points = {}\n",
        "    for point in contour:\n",
        "        point = point[0]\n",
        "        d = math.hypot(point[0] - center[0], point[1] - center[1])\n",
        "        distances.append(d)\n",
        "        distances_to_points[d] = point\n",
        "    distances = sorted(distances)\n",
        "    return [distances_to_points[distances[-1]], distances_to_points[distances[-2]]]\n",
        "\n",
        "\n",
        "def line_intersection(line1, line2):\n",
        "    x_diff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
        "    y_diff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1])\n",
        "\n",
        "    def det(a, b):\n",
        "        return a[0] * b[1] - a[1] * b[0]\n",
        "\n",
        "    div = det(x_diff, y_diff)\n",
        "    if div == 0:\n",
        "        return [-1, -1]\n",
        "\n",
        "    d = (det(*line1), det(*line2))\n",
        "    x = det(d, x_diff) / div\n",
        "    y = det(d, y_diff) / div\n",
        "    return [int(x), int(y)]\n",
        "\n",
        "\n",
        "def extend(a, b, length, int_represent=False):\n",
        "    length_ab = math.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
        "    if length_ab * length <= 0:\n",
        "        return b\n",
        "    result = [b[0] + (b[0] - a[0]) / length_ab * length, b[1] + (b[1] - a[1]) / length_ab * length]\n",
        "    if int_represent:\n",
        "        return [int(result[0]), int(result[1])]\n",
        "    else:\n",
        "        return result\n",
        "\n",
        "\n",
        "def extract(frame, debug=False):\n",
        "    #output = frame.copy()\n",
        "    output = np.zeros([frame.shape[0],frame.shape[1],4], dtype=np.uint8)\n",
        "\n",
        "    # Remove noise and unnecessary contours from frame\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.bilateralFilter(gray, 11, 17, 17)\n",
        "    gray = cv2.GaussianBlur(gray, (BLUR_VALUE, BLUR_VALUE), 0)\n",
        "    edged = cv2.Canny(gray, 30, 200)\n",
        "\n",
        "    contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    squares = []\n",
        "    square_indices = []\n",
        "\n",
        "    i = 0\n",
        "    for c in contours:\n",
        "        # Approximate the contour\n",
        "        peri = cv2.arcLength(c, True)\n",
        "        area = cv2.contourArea(c)\n",
        "        approx = cv2.approxPolyDP(c, 0.03 * peri, True)\n",
        "\n",
        "        # Find all quadrilateral contours\n",
        "        if len(approx) == 4:\n",
        "            # Determine if quadrilateral is a square within SQUARE_TOLERANCE\n",
        "            if area > 25 and 1 - SQUARE_TOLERANCE < math.fabs((peri / 4) ** 2) / area < 1 + SQUARE_TOLERANCE and count_children(hierarchy[0], i) >= 2 and has_square_parent(hierarchy[0], square_indices, i) is False:\n",
        "                squares.append(approx)\n",
        "                square_indices.append(i)\n",
        "        i += 1\n",
        "\n",
        "    main_corners = []\n",
        "    east_corners = []\n",
        "    south_corners = []\n",
        "    tiny_squares = []\n",
        "    rectangles = []\n",
        "    # Determine if squares are QR codes\n",
        "    for square in squares:\n",
        "        area = cv2.contourArea(square)\n",
        "        center = get_center(square)\n",
        "        peri = cv2.arcLength(square, True)\n",
        "\n",
        "        similar = []\n",
        "        tiny = []\n",
        "        for other in squares:\n",
        "            if square[0][0][0] != other[0][0][0]:\n",
        "                # Determine if square is similar to other square within AREA_TOLERANCE\n",
        "                if math.fabs(area - cv2.contourArea(other)) / max(area, cv2.contourArea(other)) <= AREA_TOLERANCE:\n",
        "                    similar.append(other)\n",
        "                elif peri / 4 / 2 > cv2.arcLength(other, True) / 4:\n",
        "                    tiny.append(other)\n",
        "\n",
        "        if len(similar) >= 2:\n",
        "            distances = []\n",
        "            distances_to_contours = {}\n",
        "            for sim in similar:\n",
        "                sim_center = get_center(sim)\n",
        "                d = math.hypot(sim_center[0] - center[0], sim_center[1] - center[1])\n",
        "                distances.append(d)\n",
        "                distances_to_contours[d] = sim\n",
        "            distances = sorted(distances)\n",
        "            closest_a = distances[-1]\n",
        "            closest_b = distances[-2]\n",
        "\n",
        "            # Determine if this square is the top left QR code indicator\n",
        "            if max(closest_a, closest_b) < cv2.arcLength(square, True) * 2.5 and math.fabs(closest_a - closest_b) / max(closest_a, closest_b) <= DISTANCE_TOLERANCE:\n",
        "                # Determine placement of other indicators (even if code is rotated)\n",
        "                angle_a = get_angle(get_center(distances_to_contours[closest_a]), center)\n",
        "                angle_b = get_angle(get_center(distances_to_contours[closest_b]), center)\n",
        "                if angle_a < angle_b or (angle_b < -90 and angle_a > 0):\n",
        "                    east = distances_to_contours[closest_a]\n",
        "                    south = distances_to_contours[closest_b]\n",
        "                else:\n",
        "                    east = distances_to_contours[closest_b]\n",
        "                    south = distances_to_contours[closest_a]\n",
        "                midpoint = get_midpoint(get_center(east), get_center(south))\n",
        "                # Determine location of fourth corner\n",
        "                # Find closest tiny indicator if possible\n",
        "                min_dist = 10000\n",
        "                t = []\n",
        "                tiny_found = False\n",
        "                if len(tiny) > 0:\n",
        "                    for tin in tiny:\n",
        "                        tin_center = get_center(tin)\n",
        "                        d = math.hypot(tin_center[0] - midpoint[0], tin_center[1] - midpoint[1])\n",
        "                        if d < min_dist:\n",
        "                            min_dist = d\n",
        "                            t = tin\n",
        "                    tiny_found = len(t) > 0 and min_dist < peri\n",
        "\n",
        "                diagonal = peri / 4 * 1.41421\n",
        "\n",
        "                if tiny_found:\n",
        "                    # Easy, corner is just a few blocks away from the tiny indicator\n",
        "                    tiny_squares.append(t)\n",
        "                    offset = extend(midpoint, get_center(t), peri / 4 * 1.41421)\n",
        "                else:\n",
        "                    # No tiny indicator found, must extrapolate corner based off of other corners instead\n",
        "                    farthest_a = get_farthest_points(distances_to_contours[closest_a], center)\n",
        "                    farthest_b = get_farthest_points(distances_to_contours[closest_b], center)\n",
        "                    # Use sides of indicators to determine fourth corner\n",
        "                    offset = line_intersection(farthest_a, farthest_b)\n",
        "                    if offset[0] == -1:\n",
        "                        # Error, extrapolation failed, go on to next possible code\n",
        "                        continue\n",
        "                    offset = extend(midpoint, offset, peri / 4 / 7)\n",
        "                    if debug:\n",
        "                        cv2.line(output, (farthest_a[0][0], farthest_a[0][1]), (farthest_a[1][0], farthest_a[1][1]), (0, 0, 255), 4)\n",
        "                        cv2.line(output, (farthest_b[0][0], farthest_b[0][1]), (farthest_b[1][0], farthest_b[1][1]), (0, 0, 255), 4)\n",
        "\n",
        "                # Append rectangle, offsetting to farthest borders\n",
        "                rectangles.append([extend(midpoint, center, diagonal / 2, True), extend(midpoint, get_center(distances_to_contours[closest_b]), diagonal / 2, True), offset, extend(midpoint, get_center(distances_to_contours[closest_a]), diagonal / 2, True)])\n",
        "                east_corners.append(east)\n",
        "                south_corners.append(south)\n",
        "                main_corners.append(square)\n",
        "\n",
        "    codes = []\n",
        "    i = 0\n",
        "    for rect in rectangles:\n",
        "        i += 1\n",
        "        # Draw rectangle\n",
        "        vrx = np.array((rect[0], rect[1], rect[2], rect[3]), np.int32)\n",
        "        vrx = vrx.reshape((-1, 1, 2))\n",
        "        cv2.polylines(output, [vrx], True, (0, 255, 255, 1), 2)\n",
        "        # Warp codes and draw them\n",
        "        wrect = np.zeros((4, 2), dtype=\"float32\")\n",
        "        wrect[0] = rect[0]\n",
        "        wrect[1] = rect[1]\n",
        "        wrect[2] = rect[2]\n",
        "        wrect[3] = rect[3]\n",
        "        dst = np.array([\n",
        "            [0, 0],\n",
        "            [WARP_DIM - 1, 0],\n",
        "            [WARP_DIM - 1, WARP_DIM - 1],\n",
        "            [0, WARP_DIM - 1]], dtype=\"float32\")\n",
        "        warp = cv2.warpPerspective(frame, cv2.getPerspectiveTransform(wrect, dst), (WARP_DIM, WARP_DIM))\n",
        "        # Increase contrast\n",
        "        warp = cv2.bilateralFilter(warp, 11, 17, 17)\n",
        "        warp = cv2.cvtColor(warp, cv2.COLOR_BGR2GRAY)\n",
        "        small = cv2.resize(warp, (SMALL_DIM, SMALL_DIM), 0, 0, interpolation=cv2.INTER_CUBIC)\n",
        "        _, small = cv2.threshold(small, 100, 255, cv2.THRESH_BINARY)\n",
        "        codes.append(small)\n",
        "       # if debug:\n",
        "            #cv2.imshow(\"Warped: \" + str(i), small)\n",
        "\n",
        "    if debug:\n",
        "        # Draw debug information onto frame before outputting it\n",
        "        cv2.drawContours(output, squares, -1, (5, 5, 5, 1), 2)\n",
        "        cv2.drawContours(output, main_corners, -1, (0, 0, 128,1), 2)\n",
        "        cv2.drawContours(output, east_corners, -1, (0, 128, 0,1), 2)\n",
        "        cv2.drawContours(output, south_corners, -1, (128, 0, 0,1), 2)\n",
        "        cv2.drawContours(output, tiny_squares, -1, (128, 128, 0,1), 2)\n",
        "\n",
        "    return codes, output"
      ],
      "metadata": {
        "id": "vrYUjxGBkjF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpA68lTrcvZs"
      },
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_xcqQZKYzAj"
      },
      "source": [
        "## Initialise Webcam Videos\n",
        "- start a video stream using our webcam as input\n",
        "- run each frame through detection\n",
        "- create an overlay image\n",
        "- overlay bbx video stream\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "dQDwvcpmkOxI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nkSnkbkk4cC",
        "outputId": "003ee782-8dbc-41a7-b8e4-bc776137224a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "   \n",
        "    #extract code\n",
        "    codes, frame = extract(img, False)\n",
        " \n",
        "    \n",
        "    frame[:,:,3] = (frame.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(frame)\n",
        "\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imshow(cv2.cvtColor(codes[0], cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "9yB7x5-MqKYP",
        "outputId": "a5264485-a413-48d0-ddc0-0ac26707ddd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADOElEQVR4nO3dQYqDQBBA0ekh979yzwWChjEdv+a9rRsDfgos7Iw55w/Q83v2DQDPiROixAlR4oQocULUY+viGON2r3K9nb62McbZt/B2c86nP8rkhChxQpQ4IUqcECVOiBInRIkTojb3nHuKO8M77sF43Z2eSZMTosQJUeKEKHFClDghSpwQdWiVsmXlSqP4upy+qz2TJidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghatnRmI6vpOZqz6TJCVHihChxQpQ4IUqcECVOiDq0Sln5r03wH3d6Jk1OiBInRIkTosQJUeKEKHFClDghaux8RnOtb2zgmp4uZ01OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHqsXVxjPGp+/iYOefZtwAvMTkhSpwQJU6IEidEiROixAlR4oSozT3nnuLO8I67Wb6TyQlR4oQocUKUOCFKnBAlTog6tErZsnKlUVzhwLuZnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSoZUdjOr4SjjE5IUqcECVOiBInRIkTosQJUYdWKSv/SQy+nckJUeKEKHFClDghSpwQJU6IEidEDZ92QZPJCVHihChxQpQ4IUqcECVOiPoD5Bgw1/20xy8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}